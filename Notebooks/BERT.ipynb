{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65434aa0-9f24-4080-baff-39a0c5c1cbfe",
   "metadata": {},
   "source": [
    "# Modelo de BERT\n",
    "\n",
    "El modelo de BERT (Bidirectional Encoder Representations from Transformers) propone un modelo del lenguaje pre-entrenado que tome en consideración todo el contexto de una palabra. Para esto hace uso del enmascaramiento y los mecanismos de auto-atención de los Transformers. Sin embargo, este modelo se enfoca únicamente en el encoder, dejando de lado el decoder. Al hacer esto, el modelo obtenido es más simple que los modelos de Transformer encoder-decoder. El utilizar sólo el encoder está motivado por el hecho de que la salida del modelo serán las probabilidades de las representaciones del vocabulario de entrada. No existe un vocabulario de salida, por tanto, los mecanismos del decoder no son necesarios.\n",
    "    \n",
    "El modelo de BERT se basa en un modelo del lenguaje enmascarado, lo que permite que las representaciones obtenidas contengan información tanto del contexto a la izquierda como del contexto a la derecha, y las relaciones entre todos los elementos del contexto. Se proponen dos fases para obtener las representaciones a partir de BERT: 1) Pre-entrenar el modelo para obtener representaciones de los tokens en el corpus; y 2) ajuste fino (\\textit{fine tuning}) de estas representaciones.\n",
    "    \n",
    "El <b>pre-entrenamiento</b> consiste en entrenar el modelo de BERT (el encoder del Transformer) en un modelo del lenguaje enmascarado. Esto implica que los datos del entrenamiento no están etiquetados y, por tanto, se trata de un modelo de aprendizaje no-supervisado (o auto-supervisado). Además de entrenar un modelo del lenguaje, se propone que en esta fase el modelo se entrene en una tarea de predicción de la siguiente secuencia; es decir, se entrenará el modelo para que sea capaz de predecir si, dada una segunda oración, esa oración es la oración que sigue a la oración actual o no. En este caso, se trata de una tarea supervisada, pero las etiquetas (es o no es la siguiente oración) se obtienen a partir de los datos de corpus, por lo que tampoco se requiere supervisión humana.\n",
    "\n",
    "Por otra parte, el <b>ajuste fino</b> consiste en utilizar las representaciones obtenidas en el modelo de BERT para inicializar los parámetros de un modelo de aprendizaje que se enfoque en una tarea supervisada. En este caso, se trabajará con datos etiquetados. Para realizar un ajuste fino de los parámetros se tomará, por ejemplo, una tarea de detección de spam, donde se cuente con datos etiquetados (es o no es spam). Las representaciones de BERT se utilizarán para inicializar algunos de los parámetros del modelo: por ejemplo, pueden servir para iniciar una matriz de embeddings. De esta forma, las representaciones de BERT se ajustarán al problema en cuestión para, así, obtener nuevas representaciones que sean particulares de esa tarea.\n",
    "\n",
    "### Pre-entrenamiento\n",
    "\n",
    "El pre-entrenamiento del modelo de BERT consiste en dos tareas:\n",
    "    \n",
    "\\begin{description}\n",
    "\n",
    "<b><title>Modelo del lenguaje enmascarado:</title></b> Como señalamos un modelo del lenguaje enmascarado consiste en determinar una medida de probabilidad en base a un vocabulario de tokens $\\Sigma$. La medida de probabilidad busca determinar la probabilidad de una palabra dada el contexto. Para esto, se enmascara la palabra $w^{(t)}$ y se estima la probabilidad: \n",
    "$$p(Mask = w^{(t)} |w^{(1)}...w^{(t-1)} Mask . w^{(t+1)}...w^{(T)} )$$ \n",
    "Dada una cadena de entrada $w^{(1)} w^{(2)} ... w^{(T)} \\in \\Sigma^*$, el enmascaramiento se realiza de manera aleatoria: es decir, se elige un número $t \\in \\{1,...,T\\}$ y se enmascara la palabra en ese posición. En particular, \\cite{devlin2018bert} proponen que por cada secuencia se enmascare el 15\\% de los tokens; así, si se cuenta con $T=20$ tokens en una cadena, se enmascararán 3 tokens, seleccionados de manera aleatoria. \n",
    "\n",
    "Sin embargo, este régimen de entrenamiento puede afectar al ajuste fino, puesto que la etiqueta <tt>Mask</tt> del modelo del lenguaje no aparecerá en las cadenas durante el proceso de ajuste-fino y por tanto las cadenas de entrada en el pre-entrenamiento y durante el ajuste fino pueden presentar diferencias. Para mitigar esto, los autores proponen utilizar un esquema en que no siempre se enmascaren los tokens. Este esquema se basa en tres casos, al seleccionar un token de manera aleatoria se puede hacer lo siguiente:\n",
    "\n",
    "1. Sustituirlo por la etiqueta <tt>Mask</tt> (enmascarar el token). Esto se realizará el 80\\% de las veces.\n",
    "2. Sustituir el token elegido por un token aleatorio del vocabulario. Esto se realizará el 10\\% de las veces. \n",
    "3. Mantener al token (sin ningún tipo de sustitución). Este último caso pasará el 10\\% de las veces.\n",
    "\n",
    "Por ejemplo, asumamos que tenemos la cadena \"la niña miraba por la ventana\" y que aleatoriamente se selecciona el token en la posición 3, esto es se selecciona el token \"miraba\". Puede pasar, por tanto, que:\n",
    "\n",
    "1. Se enmascare la palabra (80\\% de las veces): <p><div class='center-text'>`la niña [MASK] por la ventana`</div></p>\n",
    "2. Se sustituya por un token aleatorio, por ejemplo \"salta\" (10\\% de las veces): <p><div class='center-text'>`la niña salta por la ventana`</div></p>\n",
    "3. Se mantiene el token (10\\% de las veces): <p><div class='center-text'>`la niña miraba por la ventana`</div></p>\n",
    "    \n",
    "    \n",
    "El modelo del lenguaje buscará predecir el token que ha sido enmascarado, cuando éste sea el caso. Para cada token, la salida será la probabilidad del mismo token; si el token está enmascarado, se buscará predecir la palabra que puede ocupar ese lugar con mayor probabilidad. Se suelen utilizar etiquetas extras <tt>CLS</tt> (al principio de la oración) y <tt>Sep</tt> (al final de la oración) que parecerían jugar un papel similar a las etiquetas de inicio y final. Sin embargo, desempeñan un papel diferente. La etiqueta <tt>Sep</tt> es un separador que indica donde termina una oración y empieza otra y será de gran importancia para la tarea de predicción de la siguiente oración. Por su parte, la etiqueta <tt>CLS</tt>, si bien se suele colocar al inicio de la cadena, busca indicar que en esa posición se encuentra una representación (un vector) que codifica a la cadena completa (en tanto esta es bidireccional) y por tanto es esta representación la que se usa para tareas de clasificación. Es decir, si queremos realizar una tarea de clasificación (como detección de spam) será el vector correspondiente a <tt>CLS</tt> el que utilizaremos como entrada para esta tarea.\n",
    "\n",
    "La Figura de abajo muestra la intuición detrás del modelo enmascarado: dada una cadena enmascarada, el modelo de BERT buscará reproducir la cadena de entrada, pero sustituyendo la etiqueta de enmascaramiento por el token adecuado. Es claro que palabras que aparezcan en contextos similares tendrán una representación similar. Detrás del modelo de BERT, entonces, podemos ver que se encuentra plasmada la hipótesis distribucional.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/BERTMLM.png\" alt=\"BERT\" width=\"600\" height=\"400\"></p>\n",
    "\n",
    "<b>Predicción de la siguiente oración</b>: Para complementar el modelo del lenguaje enmascarado, el pre-entrenamiento incluirá una tarea de predicción de la oración siguiente. Para esto, cada oración terminará con una etiqueta <tt>Sep</tt> que funciona como un separador entre la oración actual y la siguiente. En general, el modelo de BERT toma como entrada dos oraciones consecutivas (y no sólo una como otros modelos). De esta forma, una cadena de entrada estará conformada por dos oraciones, por ejemplo:\n",
    "\n",
    "    `[CLS] fue a la tienda [SEP] compró un litro de leche [SEP]`\n",
    "\n",
    "La primera oración es \"fue a la tienda\" seguida de la oración \"compró un litro de leche\". Estas dos oraciones conformaría, dentro del corpus, dos oraciones separadas. La etiqueta <tt>Sep</tt> indica esto. Al entrenar de esta forma, el modelo toma en cuenta mayor contexto y encadena las oraciones consecutivamente. Así por ejemplo, la siguiente entrada podría ser:\n",
    "\n",
    "    `[CLS] compró un litro de leche [SEP] se fue de allí [SEP]`\n",
    "\n",
    "A partir de estas dos cadenas de entrada, el modelo es capaz de relacionar la oración \"se fue de allí\" con la oración \"fue a la tienda\" a partir de la oración intermedia (\"compró un litro de leche\"). A estas cadenas, además, se les aplica enmascaramiento. Pero además del modelo del lenguaje, se propone que el pre-entrenamiento realice una predicción. Se predice si la oración que viene después de <tt>Sep</tt> es la oración siguiente (dentro del corpus) o si no lo es. Por ejemplo, las dos oraciones anteriores serían clasificadas como $IsNext$ en tanto corresponde a una secuencia correcta de oraciones dentro del corpus.\n",
    "\n",
    "Para generar ejemplos negativos, se emparejan secuencias de manera aleatoria. Por ejemplo, dada la oración \"fue a la tienda\", se selecciona aleatoriamente otra oración no consecutiva dentro del corpus. A estas parejas de oraciones se les etiqueta como $NotNext$. De tal forma que el pre-entrenamiento realiza una calificación binaria entre dos series de etiquetas $IsNext$ y $NotNext$.\n",
    "\n",
    "Para identificar de manera adecuada las posiciones de las sentencias se utiliza una codificación que indica si un token pertenece a la primera o segunda oración. Esta codificación es adicional a la codificación posicional de los Transformers y la revisaremos a continuación.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a4789-e017-432d-aa80-cf1ee38bdcfb",
   "metadata": {},
   "source": [
    "## Implementación del modelo de BERT\n",
    "\n",
    "La implementación del modelo de BERT utiliza únicamente la parte de decodificador del Transformer, por lo que utiliza la auto-atención para las cabezas del modelo. La implementación que se sigue esmuy simple y consta de las siguientes subcapas:\n",
    "\n",
    "1. Embeddings y codificación posicional\n",
    "2. Cabezas de auto-atención\n",
    "3. Suma y normalizción\n",
    "\n",
    "Al final, la clase regresará el vector de clasificación <tt>cls</tt> así como los vectores contextualizados que salen por cada token de entrada. Esto permite que el modelo se pueda utilizar tanto para tareas de clasificación como para modelos del lenguaje enmascarados, adaptando una capa de salida que sea pertinente para cada caso. En este caso, descartamos la subcapa de feedforward, la cual se puede integrar como un ejercicio para comparar el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c95c0f-c012-4ba2-9f14-d953e87fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from transformers_functions import *\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, heads=3, p=0.3):\n",
    "        super().__init__()\n",
    "        #Embeddings y codificación posicional\n",
    "        self.embs = nn.Embedding(input_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        #Cabezas de atención\n",
    "        self.att =  nn.ModuleList([deepcopy(SelfAttention(d_model)) for _ in range(heads)])\n",
    "        #Capa lineal\n",
    "        self.lin = nn.Linear(heads*d_model, d_model)\n",
    "        #Auxiliares\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(p)\n",
    "        self.drop2 = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Creación de entradas\n",
    "        x_e = self.embs(x)\n",
    "        x_e += self.pos(x_e)\n",
    "        x_e = self.drop1(self.norm1(x_e))\n",
    "\n",
    "        #Aplicación de cabezas de atención\n",
    "        head_att = [head(x_e) for head in self.att]\n",
    "        self.att_weights = [head[1] for head in head_att]\n",
    "        heads = [head[0] for head in head_att]\n",
    "\n",
    "        #Aplanamiento de cabezas\n",
    "        multi_heads = torch.cat(heads, dim=-1)\n",
    "        h = self.lin(multi_heads)\n",
    "        h = self.drop2(self.norm1(h) + x_e)\n",
    "        \n",
    "        #Vector de clasificación\n",
    "        cls = h[:, 0]\n",
    "\n",
    "        return cls, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56255cb0-d749-40ca-8463-672a77aa17e1",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "\n",
    "Para probar el modelo, hacemos uso de los datos de clasificación de texto en sentimiento sobre el Coronavirus. Estos datos se encuentran disponibles en la página de Kaggle: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification.\n",
    "Estos datos cuentan con etiquetas en 5 sentimientos, desde muy negativo hasta muy positivo, pasando por neutro.\n",
    "\n",
    "Para procesar los datos, se utiliza únicamente el texto del tweet y la clasificación. Se hace uso del símbolo <tt>cs</tt> que, usualmente, es el primer elemento que aparece en la cadena de texto; por lo que toda cadena de texto tendrá la forma:\n",
    "$$[cls] \\text{ } w_1 w_2 ... w_n$$\n",
    "Donde cada $w_i$ es un token. Todas las palabras se tokenizan utilizando el tokenizador de BERT (<tt>bert-base-uncased</t>), utilizando que se encuentra en la paquetería de transformer de Hugging Face. Finalmente, se crean los lotes con cargadores de datos para el entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcb58c0-da16-4a41-a252-6b9eb3a1c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41157 41157\n",
      "torch.Size([28809, 247]) torch.Size([28809])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cienciasia/Documentos/Proyectos/BERT_Prueba/transformers_functions.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(nn.utils.rnn.pad_sequence(x, padding_value=pad)).T #x\n"
     ]
    }
   ],
   "source": [
    "#Lectura de los datos de texto\n",
    "data = pd.read_csv('SentimentCOVID/Corona_NLP_train.csv', encoding='latin1')\n",
    "dataX = data['OriginalTweet'].tolist()\n",
    "dataY = data['Sentiment'].tolist()\n",
    "\n",
    "#Etiquetas\n",
    "labels = {'Extremely Negative':4, 'Extremely Positive':3, 'Negative':2, 'Neutral':0, 'Positive':1}\n",
    "y = [labels[l] for l in dataY]\n",
    "\n",
    "#tokenización y símbolo de clase\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "voc = vocab()\n",
    "voc['[cls]'] = 0\n",
    "tokens = [['[cls]'] + tokenizer.tokenize(data_i) for data_i in dataX]\n",
    "x = list(index(tokens, voc))\n",
    "\n",
    "print(len(x), len(y))\n",
    "\n",
    "#Cargadores para entrenamiento y test\n",
    "train_loader, test_loader = get_dataset(x, y, pad=len(voc), batch_size=256)\n",
    "\n",
    "print(train_loader.dataset.x.shape, train_loader.dataset.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cb58c-f523-40f9-b3f9-84686db276fb",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Finalmente, ya con el modelo definido y los datos cargados, se puede entrenarel modelo. Además del modelo de atención, se genera un modelo clasificador, que tomará el vector de clasificación <tt>cls</tt> para clasificar el texto en su clase correspondiente. Para el entrenamiento, se utiliza la función objetivo de entropía cruzada y el optimizador Noam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fff7cc-f671-4833-958d-3db245f1b6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBERT(\n",
       "  (embs): Embedding(20909, 128)\n",
       "  (pos): PositionalEncoding()\n",
       "  (att): ModuleList(\n",
       "    (0-2): 3 x SelfAttention(\n",
       "      (Q): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (K): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (V): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop1): Dropout(p=0.3, inplace=False)\n",
       "  (drop2): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleBERT(input_size=len(voc)+1)\n",
    "classifier = nn.Sequential(nn.Linear(128, 256), nn.ReLU(),\n",
    "                          nn.Linear(256, len(labels)), nn.Softmax(-1))\n",
    "\n",
    "model.load_state_dict(torch.load('bert_class.model', weights_only=True))\n",
    "classifier.load_state_dict(torch.load('classifier_for_bert.model', weights_only=True))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4ad7b0-b486-49d0-90be-c00d3f2e7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/113 [00:00<?, ?it/s]/tmp/ipykernel_10518/2002833018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(output, torch.tensor(yi))\n",
      "100%|█████████████████████████████████████████| 113/113 [01:36<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.3567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.3753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.3589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:48<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.3678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:42<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:42<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:41<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = NoamOptimizer(list(model.parameters())+list(classifier.parameters()), 128, init_lr=0.01, decay=1e-3, warmup=40000)\n",
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    for xi, yi in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        cls, probs = model(xi)\n",
    "        output = classifier(cls)\n",
    "        \n",
    "        loss = criterion(output, torch.tensor(yi))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592cc54-f6aa-4194-815a-b4a8839c4702",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Para determinar el desempeño del modelo se evalúa con la partición de los datos correspondiente. Se realiza un reporte de clasificación para obtener los valores de preción, exhaustividad y valor $F_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e6899d-ab59-47bb-9c95-eeb4417621ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1235/1235 [00:09<00:00, 134.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Neutral       0.83      0.50      0.62      3833\n",
      "          Positive       0.55      0.54      0.55      3521\n",
      "          Negative       0.46      0.52      0.49      2614\n",
      "Extremely Positive       0.52      0.73      0.61      1420\n",
      "Extremely Negative       0.36      0.61      0.45       960\n",
      "\n",
      "          accuracy                           0.55     12348\n",
      "         macro avg       0.54      0.58      0.54     12348\n",
      "      weighted avg       0.60      0.55      0.56     12348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "\n",
    "x_pred = []\n",
    "y_labels = []\n",
    "for xi, yi in tqdm(test_loader):\n",
    "    x_pred += list(classifier(model(xi)[0]).argmax(1).detach().numpy())\n",
    "    y_labels += list(yi.numpy())\n",
    "\n",
    "print(classification_report(x_pred, y_labels, target_names=['Neutral', 'Positive', 'Negative',\n",
    "                                                            'Extremely Positive', 'Extremely Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896ff563-4cfb-4d65-8075-8354d00ac769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'bert_class.model')\n",
    "#torch.save(classifier.state_dict(), 'classifier_for_bert.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e56113-6cdb-4f9c-95ee-d672b2f4b20f",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "<a href=\"https://victormijangosdelacruz.github.io/MecanismosAtencion/\">Principal</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
