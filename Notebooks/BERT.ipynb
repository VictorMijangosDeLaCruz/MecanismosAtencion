{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65434aa0-9f24-4080-baff-39a0c5c1cbfe",
   "metadata": {},
   "source": [
    "# Modelo de BERT\n",
    "\n",
    "El modelo de BERT (Bidirectional Encoder Representations from Transformers) propone un modelo del lenguaje pre-entrenado que tome en consideración todo el contexto de una palabra. Para esto hace uso del enmascaramiento y los mecanismos de auto-atención de los Transformers. Sin embargo, este modelo se enfoca únicamente en el encoder, dejando de lado el decoder. Al hacer esto, el modelo obtenido es más simple que los modelos de Transformer encoder-decoder. El utilizar sólo el encoder está motivado por el hecho de que la salida del modelo serán las probabilidades de las representaciones del vocabulario de entrada. No existe un vocabulario de salida, por tanto, los mecanismos del decoder no son necesarios.\n",
    "    \n",
    "El modelo de BERT se basa en un modelo del lenguaje enmascarado, lo que permite que las representaciones obtenidas contengan información tanto del contexto a la izquierda como del contexto a la derecha, y las relaciones entre todos los elementos del contexto. Se proponen dos fases para obtener las representaciones a partir de BERT: 1) Pre-entrenar el modelo para obtener representaciones de los tokens en el corpus; y 2) ajuste fino (\\textit{fine tuning}) de estas representaciones.\n",
    "    \n",
    "El <b>pre-entrenamiento</b> consiste en entrenar el modelo de BERT (el encoder del Transformer) en un modelo del lenguaje enmascarado. Esto implica que los datos del entrenamiento no están etiquetados y, por tanto, se trata de un modelo de aprendizaje no-supervisado (o auto-supervisado). Además de entrenar un modelo del lenguaje, se propone que en esta fase el modelo se entrene en una tarea de predicción de la siguiente secuencia; es decir, se entrenará el modelo para que sea capaz de predecir si, dada una segunda oración, esa oración es la oración que sigue a la oración actual o no. En este caso, se trata de una tarea supervisada, pero las etiquetas (es o no es la siguiente oración) se obtienen a partir de los datos de corpus, por lo que tampoco se requiere supervisión humana.\n",
    "\n",
    "Por otra parte, el <b>ajuste fino</b> consiste en utilizar las representaciones obtenidas en el modelo de BERT para inicializar los parámetros de un modelo de aprendizaje que se enfoque en una tarea supervisada. En este caso, se trabajará con datos etiquetados. Para realizar un ajuste fino de los parámetros se tomará, por ejemplo, una tarea de detección de spam, donde se cuente con datos etiquetados (es o no es spam). Las representaciones de BERT se utilizarán para inicializar algunos de los parámetros del modelo: por ejemplo, pueden servir para iniciar una matriz de embeddings. De esta forma, las representaciones de BERT se ajustarán al problema en cuestión para, así, obtener nuevas representaciones que sean particulares de esa tarea.\n",
    "\n",
    "### Pre-entrenamiento\n",
    "\n",
    "El pre-entrenamiento del modelo de BERT consiste en dos tareas:\n",
    "    \n",
    "\\begin{description}\n",
    "\n",
    "<b><title>Modelo del lenguaje enmascarado:</title></b> Como señalamos un modelo del lenguaje enmascarado consiste en determinar una medida de probabilidad en base a un vocabulario de tokens $\\Sigma$. La medida de probabilidad busca determinar la probabilidad de una palabra dada el contexto. Para esto, se enmascara la palabra $w^{(t)}$ y se estima la probabilidad: \n",
    "$$p(Mask = w^{(t)} |w^{(1)}...w^{(t-1)} Mask . w^{(t+1)}...w^{(T)} )$$ \n",
    "Dada una cadena de entrada $w^{(1)} w^{(2)} ... w^{(T)} \\in \\Sigma^*$, el enmascaramiento se realiza de manera aleatoria: es decir, se elige un número $t \\in \\{1,...,T\\}$ y se enmascara la palabra en ese posición. En particular, \\cite{devlin2018bert} proponen que por cada secuencia se enmascare el 15\\% de los tokens; así, si se cuenta con $T=20$ tokens en una cadena, se enmascararán 3 tokens, seleccionados de manera aleatoria. \n",
    "\n",
    "Sin embargo, este régimen de entrenamiento puede afectar al ajuste fino, puesto que la etiqueta <tt>Mask</tt> del modelo del lenguaje no aparecerá en las cadenas durante el proceso de ajuste-fino y por tanto las cadenas de entrada en el pre-entrenamiento y durante el ajuste fino pueden presentar diferencias. Para mitigar esto, los autores proponen utilizar un esquema en que no siempre se enmascaren los tokens. Este esquema se basa en tres casos, al seleccionar un token de manera aleatoria se puede hacer lo siguiente:\n",
    "\n",
    "1. Sustituirlo por la etiqueta <tt>Mask</tt> (enmascarar el token). Esto se realizará el 80\\% de las veces.\n",
    "2. Sustituir el token elegido por un token aleatorio del vocabulario. Esto se realizará el 10\\% de las veces. \n",
    "3. Mantener al token (sin ningún tipo de sustitución). Este último caso pasará el 10\\% de las veces.\n",
    "\n",
    "Por ejemplo, asumamos que tenemos la cadena \"la niña miraba por la ventana\" y que aleatoriamente se selecciona el token en la posición 3, esto es se selecciona el token \"miraba\". Puede pasar, por tanto, que:\n",
    "\n",
    "1. Se enmascare la palabra (80\\% de las veces): <p><div class='center-text'>`la niña [MASK] por la ventana`</div></p>\n",
    "2. Se sustituya por un token aleatorio, por ejemplo \"salta\" (10\\% de las veces): <p><div class='center-text'>`la niña salta por la ventana`</div></p>\n",
    "3. Se mantiene el token (10\\% de las veces): <p><div class='center-text'>`la niña miraba por la ventana`</div></p>\n",
    "    \n",
    "    \n",
    "El modelo del lenguaje buscará predecir el token que ha sido enmascarado, cuando éste sea el caso. Para cada token, la salida será la probabilidad del mismo token; si el token está enmascarado, se buscará predecir la palabra que puede ocupar ese lugar con mayor probabilidad. Se suelen utilizar etiquetas extras <tt>CLS</tt> (al principio de la oración) y <tt>Sep</tt> (al final de la oración) que parecerían jugar un papel similar a las etiquetas de inicio y final. Sin embargo, desempeñan un papel diferente. La etiqueta <tt>Sep</tt> es un separador que indica donde termina una oración y empieza otra y será de gran importancia para la tarea de predicción de la siguiente oración. Por su parte, la etiqueta <tt>CLS</tt>, si bien se suele colocar al inicio de la cadena, busca indicar que en esa posición se encuentra una representación (un vector) que codifica a la cadena completa (en tanto esta es bidireccional) y por tanto es esta representación la que se usa para tareas de clasificación. Es decir, si queremos realizar una tarea de clasificación (como detección de spam) será el vector correspondiente a <tt>CLS</tt> el que utilizaremos como entrada para esta tarea.\n",
    "\n",
    "La Figura de abajo muestra la intuición detrás del modelo enmascarado: dada una cadena enmascarada, el modelo de BERT buscará reproducir la cadena de entrada, pero sustituyendo la etiqueta de enmascaramiento por el token adecuado. Es claro que palabras que aparezcan en contextos similares tendrán una representación similar. Detrás del modelo de BERT, entonces, podemos ver que se encuentra plasmada la hipótesis distribucional.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/BERTMLM.png\" alt=\"BERT\" width=\"600\" height=\"400\"></p>\n",
    "\n",
    "<b>Predicción de la siguiente oración</b>: Para complementar el modelo del lenguaje enmascarado, el pre-entrenamiento incluirá una tarea de predicción de la oración siguiente. Para esto, cada oración terminará con una etiqueta <tt>Sep</tt> que funciona como un separador entre la oración actual y la siguiente. En general, el modelo de BERT toma como entrada dos oraciones consecutivas (y no sólo una como otros modelos). De esta forma, una cadena de entrada estará conformada por dos oraciones, por ejemplo:\n",
    "\n",
    "    `[CLS] fue a la tienda [SEP] compró un litro de leche [SEP]`\n",
    "\n",
    "La primera oración es \"fue a la tienda\" seguida de la oración \"compró un litro de leche\". Estas dos oraciones conformaría, dentro del corpus, dos oraciones separadas. La etiqueta <tt>Sep</tt> indica esto. Al entrenar de esta forma, el modelo toma en cuenta mayor contexto y encadena las oraciones consecutivamente. Así por ejemplo, la siguiente entrada podría ser:\n",
    "\n",
    "    `[CLS] compró un litro de leche [SEP] se fue de allí [SEP]`\n",
    "\n",
    "A partir de estas dos cadenas de entrada, el modelo es capaz de relacionar la oración \"se fue de allí\" con la oración \"fue a la tienda\" a partir de la oración intermedia (\"compró un litro de leche\"). A estas cadenas, además, se les aplica enmascaramiento. Pero además del modelo del lenguaje, se propone que el pre-entrenamiento realice una predicción. Se predice si la oración que viene después de <tt>Sep</tt> es la oración siguiente (dentro del corpus) o si no lo es. Por ejemplo, las dos oraciones anteriores serían clasificadas como $IsNext$ en tanto corresponde a una secuencia correcta de oraciones dentro del corpus.\n",
    "\n",
    "Para generar ejemplos negativos, se emparejan secuencias de manera aleatoria. Por ejemplo, dada la oración \"fue a la tienda\", se selecciona aleatoriamente otra oración no consecutiva dentro del corpus. A estas parejas de oraciones se les etiqueta como $NotNext$. De tal forma que el pre-entrenamiento realiza una calificación binaria entre dos series de etiquetas $IsNext$ y $NotNext$.\n",
    "\n",
    "Para identificar de manera adecuada las posiciones de las sentencias se utiliza una codificación que indica si un token pertenece a la primera o segunda oración. Esta codificación es adicional a la codificación posicional de los Transformers y la revisaremos a continuación.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a4789-e017-432d-aa80-cf1ee38bdcfb",
   "metadata": {},
   "source": [
    "## Implementación del modelo de BERT\n",
    "\n",
<<<<<<< HEAD
    "La implementación del modelo de BERT utiliza únicamente la parte de decodificador del Transformer, por lo que utiliza la auto-atención para las cabezas del modelo. La implementación que se sigue esmuy simple y consta de las siguientes subcapas:\n",
    "\n",
    "1. Embeddings y codificación posicional\n",
    "2. Cabezas de auto-atención\n",
    "3. Suma y normalizción\n",
    "\n",
    "Al final, la clase regresará el vector de clasificación <tt>cls</tt> así como los vectores contextualizados que salen por cada token de entrada. Esto permite que el modelo se pueda utilizar tanto para tareas de clasificación como para modelos del lenguaje enmascarados, adaptando una capa de salida que sea pertinente para cada caso. En este caso, descartamos la subcapa de feedforward, la cual se puede integrar como un ejercicio para comparar el desempeño del modelo."
=======
    "La implementación del modelo de BERT utiliza únicamente la parte de decodificador del Transformer, por lo que utiliza la auto-atención para las cabezas del modelo. Asimismo, hace uso del símbolo <tt>CLS</tt> que, usualmente, es el primer elemento que aparece en la cadena de texto."
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 146,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "00c95c0f-c012-4ba2-9f14-d953e87fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from transformers_functions import *\n",
    "from transformers import AutoTokenizer\n",
=======
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from transformers_functions import NoamOptimizer\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Capas de proyecciones\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.K  = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.V  = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Proyección de los datos\n",
    "        query,key,value = self.Q(x),self.K(x),self.V(x)\n",
    "        # Cálculo de pesos de atención\n",
    "        scores = torch.matmul(query, key.transpose(-1,-2))/np.sqrt(self.d_model)\n",
    "        p_attn = torch.nn.functional.softmax(scores, dim = -1)\n",
    "        #Suma ponderada\n",
    "        Vs = torch.matmul(p_attn, value).reshape(x.shape)\n",
    "        \n",
    "        return Vs, p_attn\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000, warmup=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(warmup)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "                         \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        pe_slice = self.pe[:seq_len, :self.d_model]\n",
    "\n",
    "        return pe_slice.unsqueeze(0).expand(batch_size, -1, -1)\n",
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
    "\n",
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, heads=3, p=0.3):\n",
    "        super().__init__()\n",
    "        #Embeddings y codificación posicional\n",
    "        self.embs = nn.Embedding(input_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        #Cabezas de atención\n",
    "        self.att =  nn.ModuleList([deepcopy(SelfAttention(d_model)) for _ in range(heads)])\n",
    "        #Capa lineal\n",
    "        self.lin = nn.Linear(heads*d_model, d_model)\n",
    "        #Auxiliares\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(p)\n",
    "        self.drop2 = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Creación de entradas\n",
    "        x_e = self.embs(x)\n",
    "        x_e += self.pos(x_e)\n",
    "        x_e = self.drop1(self.norm1(x_e))\n",
    "\n",
    "        #Aplicación de cabezas de atención\n",
    "        head_att = [head(x_e) for head in self.att]\n",
    "        self.att_weights = [head[1] for head in head_att]\n",
    "        heads = [head[0] for head in head_att]\n",
    "\n",
    "        #Aplanamiento de cabezas\n",
    "        multi_heads = torch.cat(heads, dim=-1)\n",
    "        h = self.lin(multi_heads)\n",
    "        h = self.drop2(self.norm1(h) + x_e)\n",
    "        \n",
    "        #Vector de clasificación\n",
    "        cls = h[:, 0]\n",
    "\n",
    "        return cls, h"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "56255cb0-d749-40ca-8463-672a77aa17e1",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "\n",
    "Para probar el modelo, hacemos uso de los datos de clasificación de texto en sentimiento sobre el Coronavirus. Estos datos se encuentran disponibles en la página de Kaggle: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification.\n",
    "Estos datos cuentan con etiquetas en 5 sentimientos, desde muy negativo hasta muy positivo, pasando por neutro.\n",
    "\n",
    "Para procesar los datos, se utiliza únicamente el texto del tweet y la clasificación. Se hace uso del símbolo <tt>cs</tt> que, usualmente, es el primer elemento que aparece en la cadena de texto; por lo que toda cadena de texto tendrá la forma:\n",
    "$$[cls] \\text{ } w_1 w_2 ... w_n$$\n",
    "Donde cada $w_i$ es un token. Todas las palabras se tokenizan utilizando el tokenizador de BERT (<tt>bert-base-uncased</t>), utilizando que se encuentra en la paquetería de transformer de Hugging Face. Finalmente, se crean los lotes con cargadores de datos para el entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "cell_type": "code",
   "execution_count": 129,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "9bcb58c0-da16-4a41-a252-6b9eb3a1c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "41157 41157\n",
      "torch.Size([28809, 247]) torch.Size([28809])\n"
=======
      "41157 28809\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('SentimentCOVID/Corona_NLP_train.csv', encoding='latin1')\n",
    "dataX = data['OriginalTweet'].tolist()\n",
    "dataY = data['Sentiment'].tolist()\n",
    "\n",
    "labels = {'Extremely Negative':4, 'Extremely Positive':3, 'Negative':2, 'Neutral':0, 'Positive':1}\n",
    "y = [labels[l] for l in dataY]\n",
    "\n",
    "print(len(dataX), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd0e4192-12d0-4106-82a1-26d4699fe4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "voc = vocab()\n",
    "voc['[cls]'] = 0\n",
    "\n",
    "tokens = [['[cls]'] + tokenizer.tokenize(data_i) for data_i in dataX]\n",
    "x = list(index(tokens, voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8d29292f-4be5-4010-a503-60cabd48c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28809, 244]) torch.Size([28809])\n",
      "28809 12348\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/cienciasia/Documentos/Proyectos/BERT_Prueba/transformers_functions.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
=======
      "/tmp/ipykernel_7644/3501379650.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
      "  self.x = torch.tensor(nn.utils.rnn.pad_sequence(x, padding_value=pad)).T #x\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#Lectura de los datos de texto\n",
    "data = pd.read_csv('SentimentCOVID/Corona_NLP_train.csv', encoding='latin1')\n",
    "dataX = data['OriginalTweet'].tolist()\n",
    "dataY = data['Sentiment'].tolist()\n",
    "\n",
    "#Etiquetas\n",
    "labels = {'Extremely Negative':4, 'Extremely Positive':3, 'Negative':2, 'Neutral':0, 'Positive':1}\n",
    "y = [labels[l] for l in dataY]\n",
    "\n",
    "#tokenización y símbolo de clase\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "voc = vocab()\n",
    "voc['[cls]'] = 0\n",
    "tokens = [['[cls]'] + tokenizer.tokenize(data_i) for data_i in dataX]\n",
    "x = list(index(tokens, voc))\n",
    "\n",
    "print(len(x), len(y))\n",
    "\n",
    "#Cargadores para entrenamiento y test\n",
    "train_loader, test_loader = get_dataset(x, y, pad=len(voc), batch_size=256)\n",
    "\n",
    "print(train_loader.dataset.x.shape, train_loader.dataset.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cb58c-f523-40f9-b3f9-84686db276fb",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Finalmente, ya con el modelo definido y los datos cargados, se puede entrenarel modelo. Además del modelo de atención, se genera un modelo clasificador, que tomará el vector de clasificación <tt>cls</tt> para clasificar el texto en su clase correspondiente. Para el entrenamiento, se utiliza la función objetivo de entropía cruzada y el optimizador Noam."
=======
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pad = len(voc)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        #Carga del csv\n",
    "        self.x = torch.tensor(nn.utils.rnn.pad_sequence(x, padding_value=pad)).T #x\n",
    "        self.y = torch.tensor(y)\n",
    "        #Tamaño de los datos\n",
    "        self.size = len(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Regresa tamaño\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #Regresa un dato\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(MyDataset(x_train,y_train), batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(MyDataset(x_test,y_test), batch_size=10, shuffle=False)\n",
    "\n",
    "print(train_loader.dataset.x.shape, train_loader.dataset.y.shape)\n",
    "\n",
    "print(train_loader.dataset.size, len(y_test))\n",
    "\n",
    "\n",
    "\n",
    "#nn.utils.rnn.pad_sequence(x, padding_value=pad), dim=1)"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 159,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "f2fff7cc-f671-4833-958d-3db245f1b6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBERT(\n",
       "  (embs): Embedding(20909, 128)\n",
       "  (pos): PositionalEncoding()\n",
       "  (att): ModuleList(\n",
       "    (0-2): 3 x SelfAttention(\n",
       "      (Q): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (K): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (V): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (drop1): Dropout(p=0.3, inplace=False)\n",
       "  (drop2): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 159,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleBERT(input_size=len(voc)+1)\n",
    "classifier = nn.Sequential(nn.Linear(128, 256), nn.ReLU(),\n",
    "                          nn.Linear(256, len(labels)), nn.Softmax(-1))\n",
<<<<<<< HEAD
    "\n",
    "model.load_state_dict(torch.load('bert_class.model', weights_only=True))\n",
    "classifier.load_state_dict(torch.load('classifier_for_bert.model', weights_only=True))\n",
=======
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
    "model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 161,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "da4ad7b0-b486-49d0-90be-c00d3f2e7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "  0%|                                                   | 0/113 [00:00<?, ?it/s]/tmp/ipykernel_10518/2002833018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(output, torch.tensor(yi))\n",
      "100%|█████████████████████████████████████████| 113/113 [01:36<00:00,  1.17it/s]\n"
=======
      "  0%|                                                   | 0/226 [00:00<?, ?it/s]/tmp/ipykernel_7644/1917150452.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(output, torch.tensor(yi))\n",
      "100%|█████████████████████████████████████████| 226/226 [01:51<00:00,  2.03it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 0, Loss: 1.4078\n"
=======
      "Epoch 1, Loss: 1.6136\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [01:55<00:00,  1.96it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:51<00:00,  2.03it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.06it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.04it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:53<00:00,  1.99it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [02:00<00:00,  1.87it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [02:01<00:00,  1.86it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [02:00<00:00,  1.88it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1, Loss: 1.3841\n"
=======
      "Epoch 11, Loss: 1.5115\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [01:54<00:00,  1.97it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.06it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.10it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 2, Loss: 1.3567\n"
=======
      "Epoch 21, Loss: 1.3169\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.09it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.10it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.10it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.11it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:47<00:00,  2.09it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 3, Loss: 1.3753\n"
=======
      "Epoch 31, Loss: 1.4988\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:44<00:00,  1.08it/s]\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [02:04<00:00,  1.82it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:55<00:00,  1.96it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:55<00:00,  1.95it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.04it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.07it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:53<00:00,  2.00it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:52<00:00,  2.00it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:57<00:00,  1.92it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:55<00:00,  1.96it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 4, Loss: 1.3589\n"
=======
      "Epoch 41, Loss: 1.3608\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:48<00:00,  1.05it/s]\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [01:54<00:00,  1.98it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:53<00:00,  1.98it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:56<00:00,  1.95it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:52<00:00,  2.00it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:52<00:00,  2.01it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:54<00:00,  1.97it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:56<00:00,  1.94it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:58<00:00,  1.91it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.06it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:56<00:00,  1.95it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 5, Loss: 1.3678\n"
=======
      "Epoch 51, Loss: 1.4598\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████████| 113/113 [01:42<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:42<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [01:41<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
=======
      "100%|█████████████████████████████████████████| 226/226 [02:06<00:00,  1.79it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:58<00:00,  1.91it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.07it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:49<00:00,  2.07it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.04it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:50<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 226/226 [01:48<00:00,  2.08it/s]\n",
      " 60%|████████████████████████▍                | 135/226 [01:07<00:45,  2.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m classifier(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, torch\u001b[38;5;241m.\u001b[39mtensor(yi))\n\u001b[0;32m---> 15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_subclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_subclasses\u001b[38;5;241m.\u001b[39mfake_tensor\u001b[38;5;241m.\u001b[39mFakeTensor)\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m skip_data\n\u001b[1;32m    522\u001b[0m ):\n\u001b[1;32m    523\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 525\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    526\u001b[0m     storage,\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_offset(),\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride(),\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad,\n\u001b[1;32m    531\u001b[0m     backward_hooks,\n\u001b[1;32m    532\u001b[0m )  \u001b[38;5;66;03m# previously was self._backward_hooks\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(storage, torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mUntypedStorage):\n\u001b[1;32m    535\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    244\u001b[0m     tensors: _TensorOrTensorsOrGradEdge,\n\u001b[1;32m    245\u001b[0m     grad_tensors: Optional[_TensorOrTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m     inputs: Optional[_TensorOrTensorsOrGradEdge] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    250\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the sum of gradients of given tensors with respect to graph leaves.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If any of ``tensors``\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    are non-scalar (i.e. their data has more than one element) and require\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    gradient, then the Jacobian-vector product would be computed, in this\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    case the function additionally requires specifying ``grad_tensors``.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    It should be a sequence of matching length, that contains the \"vector\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    in the Jacobian-vector product, usually the gradient of the differentiated\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    function w.r.t. corresponding tensors (``None`` is an acceptable value for\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    all tensors that don't need gradient tensors).\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    This function accumulates gradients in the leaves - you might need to zero\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    ``.grad`` attributes or set them to ``None`` before calling it.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    See :ref:`Default gradient layouts<default-grad-layouts>`\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    for details on the memory layout of accumulated gradients.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m        Using this method with ``create_graph=True`` will create a reference cycle\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m        between the parameter and its gradient which can cause a memory leak.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m        We recommend using ``autograd.grad`` when creating the graph to avoid this.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m        If you have to use this function, make sure to reset the ``.grad`` fields of your\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        parameters to ``None`` after use to break the cycle and avoid the leak.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m        in a user-specified CUDA stream context, see\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m        When ``inputs`` are provided and a given input is not a leaf,\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m        It is an implementation detail on which the user should not rely.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m        tensors (Sequence[Tensor] or Tensor or Sequence[GradientEdge] or GradientEdge): Tensors of which\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m            the derivative will be computed.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The \"vector\" in\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            the Jacobian-vector product, usually gradients w.r.t. each element of\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m            corresponding tensors. None values can be specified for scalar Tensors or\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m            ones that don't require grad. If a None value would be acceptable for all\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m            grad_tensors, then this argument is optional.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m            will be freed. Note that in nearly all cases setting this option to ``True``\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m            is not needed and often can be worked around in a much more efficient\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m            way. Defaults to the value of ``create_graph``.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m        create_graph (bool, optional): If ``True``, graph of the derivative will\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m            be constructed, allowing to compute higher order derivative products.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m            Defaults to ``False``.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m        inputs (Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional): Inputs w.r.t. which the gradient\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m            be will accumulated into ``.grad``. All other Tensors will be ignored. If\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m            not provided, the gradient is accumulated into all the leaf Tensors that\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m            were used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward() called inside a functorch transform. This is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported, please use functorch.grad or functorch.vjp instead \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor call backward() outside of functorch transforms.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallow_mutation_on_saved_tensors\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[\n\u001b[1;32m    736\u001b[0m     _AllowMutationOnSavedContext, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m ]:\n\u001b[1;32m    738\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager under which mutating tensors saved for backward is allowed.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m    Under this context manager, tensors saved for backward are cloned on mutation,\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m    so the original version can still be used during backward. Normally, mutating a tensor\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    saved for backward will result in an error raised when it's used during backward.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;124;03m    To ensure the correct behavior, both the forward and backward should be run under\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    the same context manager.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m        An _AllowMutationOnSavedContext object storing the state managed by this\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;124;03m        context manager. This object can be useful for debugging purposes. The state\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m        managed by the context manager is automatically cleared upon exiting.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m        >>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m        ...     # forward\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m        ...     a = torch.ones(2, 3, requires_grad=True)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m        ...     b = a.clone()\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m        ...     out = (b**2).sum()\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m        ...     b.sin_()\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m        ...     # backward\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m        ...     out.sum().backward()\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m        ...\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m        tensor([[0.8415, 0.8415, 0.8415],\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m                [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _allow_mutation_on_saved_tensors_enabled\n\u001b[1;32m    769\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m _AllowMutationOnSavedContext()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = NoamOptimizer(list(model.parameters())+list(classifier.parameters()), 128, init_lr=0.01, decay=1e-3, warmup=40000)\n",
    "epochs = 10\n",
=======
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = NoamOptimizer(list(model.parameters())+list(classifier.parameters()), 128, init_lr=0.01, decay=1e-3, warmup=10000)\n",
    "epochs = 100\n",
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
    "\n",
    "model.train()\n",
    "classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    for xi, yi in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        cls, probs = model(xi)\n",
    "        output = classifier(cls)\n",
    "        \n",
    "        loss = criterion(output, torch.tensor(yi))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
<<<<<<< HEAD
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592cc54-f6aa-4194-815a-b4a8839c4702",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Para determinar el desempeño del modelo se evalúa con la partición de los datos correspondiente. Se realiza un reporte de clasificación para obtener los valores de preción, exhaustividad y valor $F_1$."
=======
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 177,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "34e6899d-ab59-47bb-9c95-eeb4417621ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████████████████████████████████| 1235/1235 [00:09<00:00, 134.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Neutral       0.83      0.50      0.62      3833\n",
      "          Positive       0.55      0.54      0.55      3521\n",
      "          Negative       0.46      0.52      0.49      2614\n",
      "Extremely Positive       0.52      0.73      0.61      1420\n",
      "Extremely Negative       0.36      0.61      0.45       960\n",
      "\n",
      "          accuracy                           0.55     12348\n",
      "         macro avg       0.54      0.58      0.54     12348\n",
      "      weighted avg       0.60      0.55      0.56     12348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
=======
      "100%|███████████████████████████████████████| 1235/1235 [00:13<00:00, 93.84it/s]\n"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from sklearn.metrics import classification_report\n",
    "\n",
=======
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
    "model.eval()\n",
    "classifier.eval()\n",
    "\n",
    "x_pred = []\n",
    "y_labels = []\n",
    "for xi, yi in tqdm(test_loader):\n",
    "    x_pred += list(classifier(model(xi)[0]).argmax(1).detach().numpy())\n",
    "    y_labels += list(yi.numpy())\n",
    "\n",
<<<<<<< HEAD
    "print(classification_report(x_pred, y_labels, target_names=['Neutral', 'Positive', 'Negative',\n",
    "                                                            'Extremely Positive', 'Extremely Negative']))"
=======
    "#print(x_pred)"
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 178,
   "id": "4985d524-3521-468b-b6e5-a776228892a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.45      0.58      4181\n",
      "           1       0.47      0.52      0.49      3055\n",
      "           2       0.34      0.49      0.40      2055\n",
      "           3       0.60      0.60      0.60      1989\n",
      "           4       0.36      0.57      0.44      1068\n",
      "\n",
      "    accuracy                           0.51     12348\n",
      "   macro avg       0.52      0.53      0.50     12348\n",
      "weighted avg       0.58      0.51      0.52     12348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(x_pred, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
>>>>>>> 0df15a53746840b38e75650741cae71447b43235
   "id": "896ff563-4cfb-4d65-8075-8354d00ac769",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert_class.model')\n",
    "torch.save(classifier.state_dict(), 'classifier_for_bert.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e56113-6cdb-4f9c-95ee-d672b2f4b20f",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "<a href=\"https://victormijangosdelacruz.github.io/MecanismosAtencion/\">Principal</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
