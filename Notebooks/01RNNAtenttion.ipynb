{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bfda7c",
   "metadata": {},
   "source": [
    "# Redes neuronales para secuencias\n",
    "\n",
    "\n",
    "En el procesamiento del lenguaje natural (PLN), el procesamiento de secuencias es esencial en tanto que las cadenas del leguaje (principalmente texto) están determinados por este tipo de estructura. Podemos pensar estos datos como dependientes del tiempo $x^{(1)} x^{(2)} \\cdots x^{(T)}$; esto es, como procesos estocásticos. Para trabajar con este tipo de datos se requieren de redes neuronales que puedan trabajar con esta estructura dentro de los datos.\n",
    "\n",
    "<b>Modelos secuenciales.</b> Una red neuronal para secuencias toma como entrada un conjunto de datos secuenciales $x^{(1)} x^{(2)} \\cdots x^{(T)}$ y cuya salida se estima como:\n",
    "$$f(x^{(1)} x^{(2)} \\cdots x^{(T)}) = \\phi\\big(W^{(out)} h^{(1:t)} + b^{(out)}\\big)$$\n",
    "donde $h^{(1:t)}$ es una representación profunda de los datos de entrada, $\\phi$ es la función de activación en la salida y $W^{(out)}$ y $b^{(out)}$ los pesos y el bias en la salida, respectivamente.\n",
    "\n",
    "Un ejemplo de este tipo de modelos son las redenes neuronales recurrentes que puede definirse como:\n",
    "\n",
    "<b>Red neuornal recurrente.</b> Es una red que representa los datos secuenciales como:\n",
    "$$h^{(t)} = g(Wh^{(t-1)} + W'x^{(t)} + b)$$\n",
    "\n",
    "Las redes recurrentes se utilizan para generar modelos que transformen una secuencia en otra secuencia a partir de una arquitectura <b>encoder-decoder</b>:\n",
    "\n",
    "<img src=\"images/EncoderDecoder.png\" width=\"500\">\n",
    "\n",
    "La codificación del encoder son los vectores $h^{(1)} h^{(2)} \\cdots h^{(T)}$; estos se utilizan para crear un vector de codificación que pasa hacia el decoder para obtener las salidas. Sin embargo, este procedimiento toma la codificación completa de la entrada para obtener todos los elementos de la secuencia de salida.\n",
    "\n",
    "## Atención en redes recurrentes\n",
    "\n",
    "Para solventar esto último, Bahdanau, Cho y Bengio (2014) introducen el mecanismo de atención. Este mecanismo se basa en enfocarse (poner \"atención\") en las entradas que tengan mayor influencia en la salida actual. De manera esquemática, la atención en las redes recurrentes se computa de la siguiente forma:\n",
    "\n",
    "1. Se calculan las representaciones del encoder $h^{(1)} h^{(2)} \\cdots h^{(T)}$.\n",
    "2. Por cada representación de la salida $s^{(t)}$ en el tiempo $t$ se estiman los <i>scores</i> como: $$sc_{t,k} = e(s^{(t-1)}, h^{(k)})$$ donde $e$ es una función de similitud.\n",
    "3. Se estima la probabilidad softmax sobre los valores de entrada, estos son los pesos de atención: $$\\alpha_{t,k} = Softmax\\big( sc_{t,k} \\big)$$\n",
    "4. Finalmente, se obtiene el vector de contexto $c^{(t)}$ a partir de la suma ponderada de las representaciones de la entrada por los pesos de atención: $$c^{(t)} = \\sum_{k=1}^T \\alpha_{t,k} h^{(k)}$$\n",
    "\n",
    "\n",
    "Visualmente, se puede representar este proceso de atención como:\n",
    "\n",
    "<img src=\"images/Attention.png\" width=\"200\">\n",
    "\n",
    "La función $e()$ para el cálculo de los scores puede determinarse de las siguientes maneras:\n",
    "\n",
    "* <b>Producto punto:</b> $$sc_{t,k} = s^{(t-1)} \\cdot h^{(k)}$$\n",
    "* <b>Forma bilineal:</b> (Luong et al, 2015) Dada una matriz de pesos $W$: $$sc_{t,k} = s^{(t-1)} W h^{(k)}$$\n",
    "* <b>MLP:</b> (Bahdanau et al, 2014) Dada una matriz de pesos $W$ y un vector de pesos $v$: $$sc_{t,k} = v^T\\tanh(W[s^{(t-1)}; h^{(k)} + b])$$\n",
    "\n",
    "A partir de estos modelos, la atención demostró tener un potencial importante en las aplicaciones de PLN y dieron pie las arquitecturas de Transformadores.\n",
    "\n",
    "\n",
    "## Referencias\n",
    "\n",
    "Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
    "\n",
    "Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "<a href=\"https://victormijangosdelacruz.github.io/MecanismosAtencion/\">Principal</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bf3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
