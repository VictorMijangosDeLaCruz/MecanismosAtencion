# Notebooks sobre atención y Transfomers

Los siguientes notebooks corresponden al curso de Mecanismos de atención en Transformers, cuyas versiones en html se pueden encontrar en este <a href="https://victormijangosdelacruz.github.io/MecanismosAtencion/">enlace</a>.

1. [Atención en RNNs](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/01RNNAtenttion.ipynb)
2. [Auto-atención](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/02SelfAttention.ipynb)
3. [Embeddings y codificación posicional](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/04Encoding.ipynb)
4. [Normalización](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/05Normalization.ipynb)
5. [Cabezas de atención](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/06AttentionHead.ipynb)
6. [Auto-atención enmascarada](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/07MaskedAttention.ipynb)
7. [Atención dispersa](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/07bSparseAtt.ipynb)
8. [Optimizador Noam](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/08Noam.ipynb)
9. [Transformador](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/09FullTransformer.ipynb)

El script con los móulos utilizados en los notebooks se puede encontar [aquí](https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/transformers.py).
